#!/usr/bin/env python

__copyright__ = 'mine260309 <mine260309 at gmail dot com>'

from calibre.web.feeds.news import BasicNewsRecipe
from calibre.ebooks.BeautifulSoup import Tag, NavigableString
from collections import OrderedDict
from contextlib import nested, closing

import re
import os, os.path

class SaveArticle:
    CUR_DIR = os.environ['PWD']
    ARTICLES_FILE = '%s/SavedArticles.txt' % CUR_DIR
    IsArticlesLoaded = False
    #SavedArticles = []
    SavedUrls = []

    def LoadArticles(self):
        print 'To load %s' % self.ARTICLES_FILE
        if not self.IsArticlesLoaded:
            try:
                lines = filter(bool,
                               [line.strip() for line
                                in open(self.ARTICLES_FILE)])
                for line in lines:
                    temp = line.split(' ')
                    url = temp[len(temp)-1]
                    self.SavedUrls.append(url)
            except Exception as e:
                print e
            self.IsArticlesLoaded = True
            print 'All aritcles: ' + str(self.SavedUrls)

    def SaveArticle(self, title, url):
        '''
        Save the artitle's title and url,
        so that they can be filtered in the future
        '''
        self.LoadArticles()
        if url in self.SavedUrls:
            print 'artitle ' + title + 'already saved'
            return
        line = title + ' ' + url
        with open(self.ARTICLES_FILE, "a") as myfile:
            myfile.write('\n')
            myfile.write(line)
            print 'added article: ' + title
        print 'Saved to %s' % self.ARTICLES_FILE

    def IsArticleSaved(self, title, url):
        self.LoadArticles()
        print 'url: %s' % url
        print 'SavedUrls: %s' % str(self.SavedUrls)
        return url in self.SavedUrls
    
class JianshuWeekly(BasicNewsRecipe):

    INDEX = 'http://jianshu.io/top/weekly'
    ARTICLE_INDEX = 'http://jianshu.io'

    title = u'简书每周热门'
    language = 'zh'
    __author__ = "Lei YU"
    description = (u'简书每周热门')
    no_stylesheets = True
    needs_subscription = False

    saveArticle = SaveArticle()

    keep_only_tags = [
      dict(name='div', attrs={'class':['article']}),
    ]

    remove_tags = [
      dict(name='div', attrs={'class':['hide']}),
    ]

    def parse_index(self):

        articles = []
        feeds = []

        soup = self.index_to_soup(self.INDEX)
        feed_title = self.tag_to_string(soup.find('title'))
        self.log.debug("Feed Title: " + feed_title)
        
        for div in soup.findAll('div', attrs={'class':'cover-img'}):
          style = div['style']
          temp = style.split('(')[1]
          self.cover_url = temp.rstrip(')')

        for ul in soup.findAll('ul', attrs={'class':'top-notes'}):
          # Main articles
          for post in ul.findAll('li'):
            h4 = post.find('h4')
            a = h4.find('a', href=True)
            title = self.tag_to_string(a)
            url = a['href']
            if "#" not in url:
              self.log.debug("Article title and url: ")
              self.log.debug(title + ": " + url)
              url = self.ARTICLE_INDEX + url
              if self.saveArticle.IsArticleSaved(title, url):
                  # Skip saved article
                  continue
              articles.append((
                {'title':title,
                 'url':url,
                 'description':'',
                 'date':''}))
              self.saveArticle.SaveArticle(title, url)

        if articles:
          feeds.append((feed_title, articles))
        else:
          raise Exception('No article, abort')
        return feeds

